{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be806b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f38f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# The base URL of the API\n",
    "base_url = 'https://api.nytimes.com/svc/archive/v1'\n",
    "\n",
    "# Your API key\n",
    "api_key = 'hpqCJQ3FUTu0y0Foot4Gl4E3oqWZZ4bc'\n",
    "\n",
    "# The years and months you want to scrape\n",
    "years = range(2010, 2020)\n",
    "months = range(1, 13)\n",
    "\n",
    "# Directory where you want to save the text files\n",
    "directory = 'D://NIKETH-DISS//Human'\n",
    "\n",
    "# Counter for the number of text files\n",
    "counter = 0\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        # If we have already saved 2000 text files, stop scraping\n",
    "        if counter >= 2000:\n",
    "            break\n",
    "\n",
    "        # Send a GET request to the API\n",
    "        response = requests.get(f'{base_url}/{year}/{month}.json', params={'api-key': api_key})\n",
    "\n",
    "        # If the GET request is successful, the status code will be 200\n",
    "        if response.status_code == 200:\n",
    "            # Parse the response as JSON\n",
    "            data = response.json()\n",
    "\n",
    "            # Loop over the articles in the response\n",
    "            for doc in data['response']['docs']:\n",
    "                # Get the text of the article\n",
    "                text = doc['lead_paragraph']\n",
    "\n",
    "                # Create a filename for each article\n",
    "                filename = os.path.join(directory, f'article{counter}.txt')\n",
    "\n",
    "                # Write the text to a file\n",
    "                with open(filename, 'w', encoding='utf-8') as f:  # Specify the encoding here\n",
    "                    f.write(text)\n",
    "\n",
    "                # Increment the counter\n",
    "                counter += 1\n",
    "\n",
    "                # If we have already saved 2000 text files, stop scraping\n",
    "                if counter >= 2000:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8435a64c",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "Request failed due to server shutdown {\n  \"error\": {\n    \"message\": \"Request failed due to server shutdown\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'Request failed due to server shutdown', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 11 Jul 2023 09:50:05 GMT', 'Content-Type': 'application/json', 'Content-Length': '141', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-davinci-002', 'openai-organization': 'user-3hoqt64bjjrbbfqvtjf2njik', 'openai-processing-ms': '6181', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249600', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '96ms', 'x-request-id': 'bad05a2e5c7619bd1ae71ce0b46eb0af', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7e5023baadbaf553-BOM', 'alt-svc': 'h3=\":443\"; ma=86400'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24148\\4222244304.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;31m# Generate the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompletion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"text-davinci-002\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;31m# Get the generated text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    151\u001b[0m         )\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         )\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             return (\n\u001b[1;32m--> 624\u001b[1;33m                 self._interpret_response_line(\n\u001b[0m\u001b[0;32m    625\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             raise self.handle_error_response(\n\u001b[0m\u001b[0;32m    688\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             )\n",
      "\u001b[1;31mAPIError\u001b[0m: Request failed due to server shutdown {\n  \"error\": {\n    \"message\": \"Request failed due to server shutdown\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'Request failed due to server shutdown', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 11 Jul 2023 09:50:05 GMT', 'Content-Type': 'application/json', 'Content-Length': '141', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-davinci-002', 'openai-organization': 'user-3hoqt64bjjrbbfqvtjf2njik', 'openai-processing-ms': '6181', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249600', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '96ms', 'x-request-id': 'bad05a2e5c7619bd1ae71ce0b46eb0af', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7e5023baadbaf553-BOM', 'alt-svc': 'h3=\":443\"; ma=86400'}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import random\n",
    "\n",
    "# Your OpenAI API key\n",
    "openai.api_key = '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'\n",
    "\n",
    "# Directory where you want to save the text files\n",
    "directory = 'D://NIKETH-DISS//AI'\n",
    "\n",
    "# The prompts you want to use to generate text\n",
    "prompts = [\n",
    "    'Once upon a time',\n",
    "    'In a galaxy far, far away',\n",
    "    'In the middle of the city',\n",
    "    'At the top of the mountain',\n",
    "    'Under the deep sea',\n",
    "    'In the heart of the jungle',\n",
    "    'During the height of the Renaissance',\n",
    "    'In the distant future',\n",
    "    'In a world where magic is real',\n",
    "    'In an ancient kingdom',\n",
    "    'In the bustling metropolis',\n",
    "    'In the quiet countryside',\n",
    "    'In the depths of space',\n",
    "    'In a post-apocalyptic world',\n",
    "    'In a world of advanced technology',\n",
    "    'In a land of myth and legend',\n",
    "    'In the corridors of power',\n",
    "    'In the realm of dreams',\n",
    "    'In a secret laboratory',\n",
    "    'In the wild west',\n",
    "    'In the roaring twenties',\n",
    "    'In a world of superheroes',\n",
    "    'In a haunted mansion',\n",
    "    'In a bustling marketplace',\n",
    "    'In a serene monastery',\n",
    "    'In a world where animals can talk',\n",
    "    'In a hidden magical school',\n",
    "    'In the midst of a great war',\n",
    "    'In a world of fantasy and adventure',\n",
    "    'In the darkest depths of the ocean',\n",
    "    'In a city of flying cars',\n",
    "    'In a world where time travel is possible',\n",
    "    'In a land where dragons roam',\n",
    "    'In a kingdom of fairies and elves',\n",
    "    'In a world where robots have taken over',\n",
    "    'In a secret spy agency',\n",
    "    'In a world where humans live on Mars',\n",
    "    'In a city of endless night',\n",
    "    'In a world where dreams can be controlled',\n",
    "    'In a realm where thoughts can be read',\n",
    "    'In a world where everyone has superpowers',\n",
    "    'In a city that never sleeps',\n",
    "    'In a world where the internet is alive',\n",
    "    'In a universe where parallel dimensions exist',\n",
    "    'In a world where music can be seen',\n",
    "    'In a land where the sun never sets',\n",
    "    'In a world where books are portals to other worlds',\n",
    "    'In a city hidden beneath the earth',\n",
    "    'In a world where the moon is a living entity',\n",
    "    'In a realm where stars are sentient',\n",
    "    'In a world where art comes to life',\n",
    "    'In a city built on clouds',\n",
    "    'In a world where emotions can be manipulated',\n",
    "    'In a land where seasons change every day',\n",
    "    'In a world where memories can be bought and sold',\n",
    "    'In a city where the past, present, and future coexist',\n",
    "    'In a world where love is a tangible force',\n",
    "    'In a land where every person has a unique ability',\n",
    "    'In a world where imagination shapes reality',\n",
    "    'In a city where every building tells a story',\n",
    "    'In a world where nature and technology are one',\n",
    "    'In a land where every creature is a mythical beast',\n",
    "    'In a world where humans and aliens coexist',\n",
    "    'In a city where every street leads to a different era',\n",
    "    'In a world where every thought creates a universe',\n",
    "    'In a land where every word spoken becomes truth',\n",
    "    'In a world where every choice changes the course of history',\n",
    "    'In a city where every persons life is a masterpiece',\n",
    "    'In a world where every dream is a prophecy',\n",
    "    'In a land where every song holds a secret',\n",
    "    'In a world where every story is real',\n",
    "    'In a city where every whisper echoes through eternity',\n",
    "    'In a world where every silence speaks volumes',\n",
    "    'In a land where every journey is an epic adventure',\n",
    "    'In a world where every mystery is a gateway to discovery',\n",
    "    'In a city where every moment is a precious memory',\n",
    "    'In a world where every heartbeat is a symphony',\n",
    "    'In a land where every breath is a miracle',\n",
    "    'In a world where every soul is a shining star',\n",
    "    'In a city where every tear is a pearl of wisdom',\n",
    "    'In a world where every smile is a ray of sunshine',\n",
    "    'In a land where every laugh is a fountain of joy',\n",
    "    'In a world where every sorrow is a seed of strength',\n",
    "    'In a city where every challenge is a stepping stone to success',\n",
    "    'In a world where every failure is a lesson learned',\n",
    "    'In a land where every victory is a testament to perseverance',\n",
    "    'In a world where every friendship is a treasure',\n",
    "    'In a city where every love is a timeless tale',\n",
    "    'In a world where every life is a journey of self-discovery',\n",
    "    'In a land where every day is a gift to be cherished',\n",
    "    'In a world where every night is a canvas of dreams',\n",
    "    'In a city where every dawn is a promise of hope',\n",
    "    'In a world where every dusk is a reflection of gratitude',\n",
    "    'In a land where every season is a celebration of life',\n",
    "    'In a world where every year is a milestone of growth',\n",
    "    'In a city where every decade is a legacy of wisdom',\n",
    "    'In a world where every century is a saga of evolution',\n",
    "    'In a land where every millennium is an epoch of enlightenment',\n",
    "    'In a world where every aeon is an era of transcendence',\n",
    "    'In a city where every infinity is an eternity of existence',\n",
    "    'In a world where every reality is a dimension of consciousness',\n",
    "    'In a land where every universe is a realm of possibilities',\n",
    "    'In a world where every cosmos is a spectrum of creation',\n",
    "    'In a city where every existence is a manifestation of divinity']\n",
    "\n",
    "\n",
    "# The maximum length of the generated text\n",
    "max_length = 400\n",
    "\n",
    "# Counter for the number of text files\n",
    "counter = 0\n",
    "\n",
    "# Generate 2000 pieces of text\n",
    "for i in range(2000):\n",
    "    # Select a random prompt\n",
    "    prompt = random.choice(prompts)\n",
    "\n",
    "    # Generate the text\n",
    "    response = openai.Completion.create(engine=\"text-davinci-002\", prompt=prompt, max_tokens=max_length)\n",
    "\n",
    "    # Get the generated text\n",
    "    text = response.choices[0].text.strip()\n",
    "\n",
    "    # Create a filename for each piece of text\n",
    "    filename = os.path.join(directory, f'text{counter}.txt')\n",
    "\n",
    "    # Write the text to a file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    # Increment the counter\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd15c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Directory where the human-written text files are saved\n",
    "human_directory = 'D://NIKETH-DISS//Human'\n",
    "\n",
    "# Directory where the AI-generated text files are saved\n",
    "ai_directory = 'D://NIKETH-DISS//AI'\n",
    "\n",
    "# Load the text files into pandas DataFrames\n",
    "human_texts = pd.DataFrame({'text': [Path(os.path.join(human_directory, filename)).read_text(encoding='utf-8-sig') for filename in os.listdir(human_directory)]})\n",
    "ai_texts = pd.DataFrame({'text': [Path(os.path.join(ai_directory, filename)).read_text(encoding='utf-8-sig') for filename in os.listdir(ai_directory)]})\n",
    "\n",
    "# Label the texts\n",
    "human_texts['label'] = 0  # 0 for human-written\n",
    "ai_texts['label'] = 1  # 1 for AI-generated\n",
    "\n",
    "# Combine the DataFrames\n",
    "texts = pd.concat([human_texts, ai_texts], ignore_index=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts['text'], texts['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the text into a matrix of token counts\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train).toarray()\n",
    "X_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# Create a neural network model\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history=model.fit(X_train, y_train, epochs=10, verbose=False, validation_data=(X_test, y_test), batch_size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85bbcf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FORGET what your parents told you: money does ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In New York, the dance New Year starts on Jan....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Resolutions come and resolutions go, and enoug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The B.C.S. begins its controversial season wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The name and spirit of Horton Foote, the Pulit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Thirty–one proposed renewable energy and power...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>An article on Dec. 27 about the death of Edwar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>The Directors Guild of America will join its a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>My last memory of Iraq was of boarding a trans...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>The struggling regional airline operator Mesa ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     FORGET what your parents told you: money does ...      0\n",
       "1     In New York, the dance New Year starts on Jan....      0\n",
       "2     Resolutions come and resolutions go, and enoug...      0\n",
       "3      The B.C.S. begins its controversial season wi...      0\n",
       "4     The name and spirit of Horton Foote, the Pulit...      0\n",
       "...                                                 ...    ...\n",
       "1995  Thirty–one proposed renewable energy and power...      0\n",
       "1996  An article on Dec. 27 about the death of Edwar...      0\n",
       "1997  The Directors Guild of America will join its a...      0\n",
       "1998  My last memory of Iraq was of boarding a trans...      0\n",
       "1999  The struggling regional airline operator Mesa ...      0\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5253210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe4344b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2208    1\n",
       "3300    1\n",
       "1309    0\n",
       "1472    0\n",
       "582     0\n",
       "       ..\n",
       "1095    0\n",
       "1130    0\n",
       "1294    0\n",
       "860     0\n",
       "3174    1\n",
       "Name: label, Length: 2669, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28651c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "920baa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                172260    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 172,271\n",
      "Trainable params: 172,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc16c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.947604775428772\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(f'Training Accuracy: {accuracy}')\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(f'Testing Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb4d8020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 75ms/step\n",
      "Predicted class: Human\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to predict the class (AI or human) for a given text\n",
    "def predict_text_class(text):\n",
    "    # Vectorize the input text\n",
    "    text_vectorized = vectorizer.transform([text]).toarray()\n",
    "\n",
    "    # Predict the class probabilities\n",
    "    class_probabilities = model.predict(text_vectorized)[0]\n",
    "\n",
    "    # Map the predicted class probabilities to class labels\n",
    "    class_labels = ['Human', 'AI']\n",
    "    predicted_class = class_labels[np.argmax(class_probabilities)]\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "text = \"In orchard's embrace, the Apple stands tall,Its crimson skin a vibrant curtain's call.Juicy nectar hides beneath its gentle sway,Nature's sweet gift, enchanting every day.\"\n",
    "predicted_class = predict_text_class(text)\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dcea62",
   "metadata": {},
   "source": [
    "# USING LSTM TO FIND THE SEQUENCE OF THE WORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ab39ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Directory where the human-written text files are saved\n",
    "human_directory = 'D://NIKETH-DISS//Human'\n",
    "\n",
    "# Directory where the AI-generated text files are saved\n",
    "ai_directory = 'D://NIKETH-DISS//AI'\n",
    "\n",
    "# Load the text files into pandas DataFrames\n",
    "human_texts = pd.DataFrame({'text': [Path(os.path.join(human_directory, filename)).read_text(encoding='utf-8-sig') for filename in os.listdir(human_directory)]})\n",
    "ai_texts = pd.DataFrame({'text': [Path(os.path.join(ai_directory, filename)).read_text(encoding='utf-8-sig') for filename in os.listdir(ai_directory)]})\n",
    "\n",
    "# Label the texts\n",
    "human_texts['label'] = 0  # 0 for human-written\n",
    "ai_texts['label'] = 1  # 1 for AI-generated\n",
    "\n",
    "# Combine the DataFrames\n",
    "texts = pd.concat([human_texts, ai_texts], ignore_index=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts['text'], texts['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the text into sequences of integers\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences so they all have the same length\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6af32ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LSTM model\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=5000, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.LSTM(100))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=10, \n",
    "                    verbose=False, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac3a5593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 304ms/step\n",
      "Predicted class: Human\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the class (AI or human) for a given text\n",
    "def predict_text_class(text):\n",
    "    # Convert the input text into sequences of integers and pad it\n",
    "    text_seq = tokenizer.texts_to_sequences([text])\n",
    "    text_padded = pad_sequences(text_seq, padding='post', maxlen=maxlen)\n",
    "\n",
    "    # Predict the class probabilities\n",
    "    class_probabilities = model.predict(text_padded)[0]\n",
    "\n",
    "    # Map the predicted class probabilities to class labels\n",
    "    class_labels = ['Human', 'AI']\n",
    "    predicted_class = class_labels[int(round(class_probabilities[0]))]\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "text = \"In orchard's embrace, the Apple stands tall,Its crimson skin a vibrant curtain's call.Juicy nectar hides beneath its gentle sway,Nature's sweet gift, enchanting every day.\"\n",
    "predicted_class = predict_text_class(text)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1477f5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           250000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               60400     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 310,501\n",
      "Trainable params: 310,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e5df2",
   "metadata": {},
   "source": [
    "# Applying RoBert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15ac465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os \n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Initialize the RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Directory where the human-written text files are saved\n",
    "human_directory = 'D://NIKETH-DISS//Human'\n",
    "\n",
    "# Directory where the AI-generated text files are saved\n",
    "ai_directory = 'D://NIKETH-DISS//AI'\n",
    "\n",
    "# Load the text files into pandas DataFrames\n",
    "human_texts = pd.DataFrame({'text': [Path(os.path.join(human_directory, filename)).read_text(encoding='utf-8-sig') for filename in os.listdir(human_directory)]})\n",
    "ai_texts = pd.DataFrame({'text': [Path(os.path.join(ai_directory, filename)).read_text(encoding='utf-8-sig') for filename in os.listdir(ai_directory)]})\n",
    "\n",
    "# Label the texts\n",
    "human_texts['label'] = 0  # 0 for human-written\n",
    "ai_texts['label'] = 1  # 1 for AI-generated\n",
    "\n",
    "# Combine the DataFrames\n",
    "texts = pd.concat([human_texts, ai_texts], ignore_index=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts['text'], texts['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5848072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Deep.ai\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaForSequenceClassification, AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create PyTorch DataLoaders\n",
    "train_dataset = TextDataset(train_encodings, list(y_train))\n",
    "test_dataset = TextDataset(test_encodings, list(y_test))\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Initialize the RoBERTa model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261500dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 [472/2669 (18%)] Loss: 0.004710:  18%|█████▋                          | 59/334 [26:18<2:39:18, 34.76s/it]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    progress_bar = tqdm(train_loader, desc='Train Epoch {}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        progress_bar.set_description('Train Epoch {} [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n",
    "            epoch, total, len(train_loader.dataset),\n",
    "            100. * total / len(train_loader.dataset), loss.item()))\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    train_accuracies.append(correct / total)\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc='Test', leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            progress_bar.set_description('Test [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n",
    "                total, len(test_loader.dataset),\n",
    "                100. * total / len(test_loader.dataset), loss.item()))\n",
    "\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    test_accuracies.append(correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54792742",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7983338",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    train_accuracies.append(correct / total)\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    test_accuracies.append(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73850776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy graphs\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to predict the class (AI or human) for a given text\n",
    "def predict_text_class(text):\n",
    "    # Tokenize the input text\n",
    "    encodings = tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "    input_ids = encodings['input_ids']\n",
    "    attention_mask = encodings['attention_mask']\n",
    "\n",
    "    # Predict the class probabilities\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    _, predicted = torch.max(outputs.logits, 1)\n",
    "\n",
    "    # Map the predicted class probabilities to class labels\n",
    "    class_labels = ['Human', 'AI']\n",
    "    predicted_class = class_labels[predicted]\n",
    "\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173294a",
   "metadata": {},
   "source": [
    "# Upgrading the dataset iusing the chat gpt for larger Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "subdir = 'data'\n",
    "if not os.path.exists(subdir):\n",
    "    os.makedirs(subdir)\n",
    "subdir = subdir.replace('\\\\','/') # needed for Windows\n",
    "\n",
    "for ds in [\n",
    "    'webtext',\n",
    "    'small-117M',  'small-117M-k40',\n",
    "    'medium-345M', 'medium-345M-k40',\n",
    "    'large-762M',  'large-762M-k40',\n",
    "    'xl-1542M',    'xl-1542M-k40',\n",
    "]:\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        filename = ds + \".\" + split + '.jsonl'\n",
    "        r = requests.get(\"https://openaipublic.azureedge.net/gpt-2/output-dataset/v1/\" + filename, stream=True)\n",
    "\n",
    "        with open(os.path.join(subdir, filename), 'wb') as f:\n",
    "            file_size = int(r.headers[\"content-length\"])\n",
    "            chunk_size = 1000\n",
    "            with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
    "                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555da4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usinf the text prompts data and the parahase data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392c93f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f56244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"D://niket new dataset//archive (2)//chatgpt_paraphrases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f35a3baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deep.ai\\AppData\\Local\\Temp\\ipykernel_15120\\3036235868.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_df['label'] = 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In what ways is the younger generation more se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you explain the correlation between Bulgar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the Russian view on the Romanovs?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The governance of the Holy See on a daily basi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Having had two visits from heads of state in 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  In what ways is the younger generation more se...      1\n",
       "1  Can you explain the correlation between Bulgar...      1\n",
       "2          What is the Russian view on the Romanovs?      1\n",
       "3  The governance of the Holy See on a daily basi...      1\n",
       "4  Having had two visits from heads of state in 2...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Create a copy of the dataframe\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Parse the 'paraphrases' column into a list of strings\n",
    "df_copy['paraphrases'] = df_copy['paraphrases'].apply(ast.literal_eval)\n",
    "\n",
    "# Create a new dataframe for the AI-generated texts\n",
    "ai_df = df_copy.explode('paraphrases')[['paraphrases']]\n",
    "ai_df.columns = ['text']\n",
    "ai_df['label'] = 1\n",
    "\n",
    "# Create a new dataframe for the human-written texts\n",
    "human_df = df[['text']]\n",
    "human_df['label'] = 0\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df_combined = pd.concat([human_df, ai_df])\n",
    "\n",
    "# Shuffle the rows\n",
    "df_combined = df_combined.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b637107f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2095985\n",
       "0     419197\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2cbab68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2012145,), (2012145,), (503037,), (503037,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "train, test = train_test_split(df_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate the features and labels\n",
    "train_texts, train_labels = train['text'], train['label']\n",
    "test_texts, test_labels = test['text'], test['label']\n",
    "\n",
    "train_texts.shape, train_labels.shape, test_texts.shape, test_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87453a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2012145, 200), (2012145,), (503037, 200), (503037,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set a vocabulary size. This is the maximum number of words that can be used.\n",
    "vocabulary_size = 20000\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Tokenize the texts\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Set a length for the sequences\n",
    "max_sequence_length = 200\n",
    "\n",
    "# Pad the sequences so that they are all the same length\n",
    "train_x = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "test_x = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "train_y = train_labels.values\n",
    "test_y = test_labels.values\n",
    "\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42cfeb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "066ee2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "embedding_dimension = 100\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "model.add(Embedding(vocabulary_size, embedding_dimension, input_length=max_sequence_length))\n",
    "\n",
    "# Add a SpatialDropout1D layer\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "# Add an LSTM layer\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# Add a Dense layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7654bb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 100)          2000000   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 200, 100)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,080,501\n",
      "Trainable params: 2,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d200fc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25152/25152 [==============================] - 90567s 4s/step - loss: 0.3071 - accuracy: 0.8756 - val_loss: 0.2847 - val_accuracy: 0.8850\n",
      "Epoch 2/5\n",
      "25152/25152 [==============================] - 12145s 483ms/step - loss: 0.2739 - accuracy: 0.8900 - val_loss: 0.2772 - val_accuracy: 0.8887\n",
      "Epoch 3/5\n",
      "25152/25152 [==============================] - 67873s 3s/step - loss: 0.2610 - accuracy: 0.8960 - val_loss: 0.2772 - val_accuracy: 0.8903\n",
      "Epoch 4/5\n",
      "25152/25152 [==============================] - 12157s 483ms/step - loss: 0.2528 - accuracy: 0.8999 - val_loss: 0.2808 - val_accuracy: 0.8911\n",
      "Epoch 5/5\n",
      "25152/25152 [==============================] - 13304s 529ms/step - loss: 0.2472 - accuracy: 0.9024 - val_loss: 0.2781 - val_accuracy: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b3103422e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_x, train_y, epochs=5, batch_size=64, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "259bcef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15720/15720 [==============================] - 8601s 547ms/step - loss: 0.2759 - accuracy: 0.8919\n",
      "Loss: 0.27593448758125305\n",
      "Accuracy: 0.8918747305870056\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_x, test_y)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaa41de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15720/15720 [==============================] - 967s 61ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.54      0.62     83981\n",
      "           1       0.91      0.96      0.94    419056\n",
      "\n",
      "    accuracy                           0.89    503037\n",
      "   macro avg       0.83      0.75      0.78    503037\n",
      "weighted avg       0.88      0.89      0.88    503037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "test_predictions = model.predict(test_x)\n",
    "\n",
    "# Convert the predictions to binary labels\n",
    "test_predictions = [1 if p > 0.5 else 0 for p in test_predictions]\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(test_y, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4941632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text):\n",
    "    # Tokenize the text\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    \n",
    "    # Pad the sequence\n",
    "    sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Make a prediction on the text\n",
    "    prediction = model.predict(sequence)\n",
    "    \n",
    "    # Convert the prediction to binary label\n",
    "    prediction = [1 if p > 0.5 else 0 for p in prediction]\n",
    "    \n",
    "    # Return the prediction\n",
    "    return \"AI\" if prediction[0] == 1 else \"Human\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e63a948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step\n",
      "AI\n"
     ]
    }
   ],
   "source": [
    "text = \" The orange is a citrus fruit known for its bright color and juicy, sweet taste. It's a rich source of vitamin C, an essential nutrient that supports immune health. Other key elements in an orange include fiber, potassium, and various antioxidants. They are eaten fresh, and their juice is often used for beverages. In addition, their zest adds a distinct flavor to baked goods. Originating in ancient China, oranges are now grown in warm climates around the globe. Interestingly, the name 'orange' also represents its color, making it unique in the color spectrum.\"\n",
    "print(predict_text(text)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "818b3a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step\n",
      "Human\n"
     ]
    }
   ],
   "source": [
    "text ='The orange is a citrus fruit known for its vivid colour and juicy, sweet taste. It s a rich supply of vitamin C, an critical nutrient that supports immune health. Other key factors in an orange consist of fiber, potassium, and various antioxidants. They are eaten fresh, and their juice is frequently used for liquids. In addition, their zest provides a distinct taste to baked goods. Originating in historic China, oranges are now grown in warm climates around the globe. Interestingly, the name orange  additionally represents its colour, making it specific inside the coloration spectrum'\n",
    "print(predict_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03cca7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text_proba(text):\n",
    "    # Tokenize the text\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    \n",
    "    # Pad the sequence\n",
    "    sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Make a prediction on the text\n",
    "    prediction_proba = model.predict(sequence)\n",
    "    \n",
    "    # Return the prediction\n",
    "    return prediction_proba[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2291875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step\n",
      "0.5090504\n"
     ]
    }
   ],
   "source": [
    "text = \"The orange is a citrus fruit known for its bright color and juicy, sweet taste. It's a rich source of vitamin C, an essential nutrient that supports immune health. Other key elements in an orange include fiber, potassium, and various antioxidants. They are eaten fresh, and their juice is often used for beverages. In addition, their zest adds a distinct flavor to baked goods. Originating in ancient China, oranges are now grown in warm climates around the globe. Interestingly, the name 'orange' also represents its color, making it unique in the color spectrum.\"\n",
    "print(predict_text_proba(text)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e7c102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 84ms/step\n",
      "0.1597737\n"
     ]
    }
   ],
   "source": [
    "text ='The orange is a citrus fruit known for its vivid colour and juicy, sweet taste. It s a rich supply of vitamin C, an critical nutrient that supports immune health. Other key factors in an orange consist of fiber, potassium, and various antioxidants. They are eaten fresh, and their juice is frequently used for liquids. In addition, their zest provides a distinct taste to baked goods. Originating in historic China, oranges are now grown in warm climates around the globe. Interestingly, the name orange  additionally represents its colour, making it specific inside the coloration spectrum'\n",
    "print(predict_text_proba(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158a3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c476d493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AI_DET_new\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('AI_DET_new')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa403c6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14964\\205223994.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create a tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Save the tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_texts' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming 'train_texts' is your training data\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "vocabulary_size = 20000\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd0f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "import tensorflow as tf\n",
    "\n",
    "loaded_model = tf.keras.models.load_model('AI_DET_new')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f50c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8656495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('my_model')\n",
    "\n",
    "# Load the tokenizer\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'POST':\n",
    "        text = request.form['text']\n",
    "        # Tokenize and pad the text\n",
    "        sequences = tokenizer.texts_to_sequences([text])\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=200)\n",
    "        # Predict with the model\n",
    "        prediction = model.predict(np.array(padded_sequences))\n",
    "        # Convert the prediction to a percentage\n",
    "        percentage = prediction[0][0] * 100\n",
    "        return render_template('index.html', percentage=percentage)\n",
    "    return render_template('index.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c85b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>AI Text Similarity</title>\n",
    "</head>\n",
    "<body>\n",
    "    <form method=\"POST\">\n",
    "        <textarea name=\"text\"></textarea>\n",
    "        <input type=\"submit\" value=\"Submit\">\n",
    "    </form>\n",
    "    {% if percentage %}\n",
    "        <p>The text is {{ percentage }}% similar to AI-generated text.</p>\n",
    "    {% endif %}\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fff0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bf6bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Deep.ai\\anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\Deep.ai\\AppData\\Local\\Temp\\ipykernel_14964\\2280534876.py\", line 14, in open_file\n",
      "    with open(filename, 'r') as file:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: ''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 370ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Load the model and the tokenizer\n",
    "model = load_model('AI_DET')\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "def open_file():\n",
    "    filename = filedialog.askopenfilename(initialdir = \"/\", title = \"Select a File\", filetypes = ((\"Text files\", \"*.txt*\"), (\"all files\", \"*.*\")))\n",
    "    with open(filename, 'r') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "        predict_text(data)\n",
    "\n",
    "def predict_text(text):\n",
    "    # Tokenize and pad the text\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    sequence = pad_sequences(sequence, maxlen=200)\n",
    "    \n",
    "    # Use the model to predict the percentage of AI content\n",
    "    prediction = model.predict(sequence)[0][0]\n",
    "    result_label.config(text=f\"Prediction: {prediction}\")\n",
    "    \n",
    "\n",
    "# Create a tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"AI Content Detector\")\n",
    "\n",
    "# Create a 'Choose File' button\n",
    "open_button = tk.Button(window, text=\"Choose File\", command=open_file)\n",
    "open_button.pack()\n",
    "\n",
    "# Create a label for showing the result\n",
    "result_label = tk.Label(window, text=\"\")\n",
    "result_label.pack()\n",
    "\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5be4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
